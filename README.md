# DeepMind-Paper-Study
Deep-RL Study
* * *

|  <center>Date</center> | <center>Week</center> | <center>Topic</center> |<center>Presenter</center> |<center>Slides</center> |<center>Paper Link</center>|<center>Keyward</center>|
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
|**<center>2020-01-03</center>** | <center>01-2 </center> | *<center>Playing Atari with Deep Reinforcement Learning</center>* |<center>김윤성</center>|[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2001_2%20Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.pptx) |*[Link](https://arxiv.org/pdf/1312.5602v1.pdf)* |<center>DQN</center> |
|**2020-01-03** | <center>01-2 </center> |*<center>Recurrent Models of Visual Attention</center>*|<center>이상현</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2001_2%20Recurrent%20Models%20of%20Visual%20Attention.pdf) |*[Link](https://arxiv.org/pdf/1406.6247.pdf)* |<center>Human-like Vision</center> |
|**2020-01-11** | <center>02-1 </center> |*<center>Deterministic Policy Gradient Algorithms</center>*|<center>이상현</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2002_1%20Deterministic%20Policy%20Gradient%20Algorithms.pdf) |*[Link](http://proceedings.mlr.press/v32/silver14.pdf)* |<center> DPG </center> |
|**2020-01-17** | <center>03-1 </center> |*<center>Continous Control with Deep Reinforcement Learning</center>*|<center>이상현</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2003_1%20Continous%20Control%20Wit%20Deep%20Reinforcement%20Learning.pdf) |*[Link](https://arxiv.org/pdf/1509.02971v2.pdf)* |<center> DDPG </center> |
|**2020-01-17** | <center>03-2 </center> |*<center>Asynchronous Methods for Deep Reinforcement Learning</center>*|<center>김윤성</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2003_1%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning.pptx) |*[Link](https://arxiv.org/pdf/1602.01783.pdf)* |<center> A3C </center> |
|**2020-02-02** | <center>04-1 </center> |*<center>Variational Information Maximization for Intrinsically Motivated Reinforcement Learning</center>*|<center>이상현</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2004_1%20Variational%20Information%20Maximisation%20for%20Intrinsically%20Motivated%20Reinforcement%20Learning.pdf) |*[Link](https://arxiv.org/pdf/1509.08731.pdf)* |<center> EmpowermentRL </center> |
|**2020-02-02** | <center>04-1 </center> |*<center>Mastering the game of Go with deep neural networks and tree search</center>*|<center>김윤성</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2004_1%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search.pptx) |*[Link](https://www.nature.com/articles/nature16961)* |<center> AlphaGo </center> |
|**2020-02-12** | <center>05-1 </center> |*<center>Continuous Deep Q-Learning with Model-based Acceleration</center>*|<center>이상현</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2005_1%20Continuous%20Deep%20Q-Learning%20with%20Model-based%20Accerlation.pdf) |*[Link](https://arxiv.org/pdf/1603.00748.pdf)* |<center> NAF, Imagination Rollout </center> |
|**2020-02-12** | <center>05-1 </center> |*<center>Reinforcement Learning with Unsupervised Auxiliary Tasks</center>*|<center>김윤성</center> |[PPT](https://github.com/niceDuckgu/DeepMind-Paper-Study/blob/master/PPT/Week%2005_1%20Reinforcement%20Learning%20with%20Unsupervised%20Auxiliary%20Tasks.pdf) |*[Link](https://arxiv.org/pdf/1611.05397.pdf)* |<center> A3C, Auxiliary Tasks </center> |



